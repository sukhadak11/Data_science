{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ba7d326-7b03-4448-98f8-8d96916ecff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Embedding, Dropout, LayerNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f2fc1b8-4545-4b2b-8219-85194eb5785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = np.arange(position)[:, np.newaxis] / np.power(\n",
    "        10000, (2 * (np.arange(d_model) // 2)) / np.float32(d_model))\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    return tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ce6a8df-1664-46d4-a8d2-2103d32d6b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert d_model % num_heads == 0\n",
    "        self.depth = d_model // num_heads\n",
    "        self.wq = Dense(d_model)\n",
    "        self.wk = Dense(d_model)\n",
    "        self.wv = Dense(d_model)\n",
    "        self.dense = Dense(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "519e3c66-25f5-4d3f-a8aa-5d635f701f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(self, x, batch_size):\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcb39f73-f860-450e-aa1a-17a2847c1a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(self, v, k, q, mask):\n",
    "                batch_size = tf.shape(q)[0]\n",
    "                q = self.wq(q)\n",
    "                k = self.wk(k)\n",
    "                v = self.wv(v)\n",
    "                q = self.split_heads(q, batch_size)\n",
    "                k = self.split_heads(k, batch_size)\n",
    "                v = self.split_heads(v, batch_size)\n",
    "                \n",
    "                attention, attention_weights = self.scaled_dot_product_attention(q, k, v, mask)\n",
    "                attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "                attention = tf.reshape(attention, (batch_size, -1, self.d_model))\n",
    "                output = self.dense(attention)\n",
    "                return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32ac35e5-6f03-435e-891e-39e49d164e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(self, q, k, v, mask):\n",
    "              matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "              dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "              scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "          \n",
    "              if mask is not None:\n",
    "                  scaled_attention_logits += (mask * -1e9)\n",
    "          \n",
    "              attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "              output = tf.matmul(attention_weights, v)\n",
    "              return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "057792ed-cad9-4847-a16f-c8540f9077f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff):\n",
    "        super(PositionwiseFeedforward, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dff = dff\n",
    "        self.dense1 = Dense(dff, activation='relu')\n",
    "        self.dense2 = Dense(d_model)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae318a34-43e5-489f-a590-1b9e928b7e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PositionwiseFeedforward(d_model, dff)\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attn_output = self.att(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6073fb0-8afb-4e62-bf88-f292509fce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, dropout_rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(\n",
    "            maximum_position_encoding, d_model)\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.enc_layers = [TransformerBlock(\n",
    "            d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)]\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training=training, mask=mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "173d6586-e33d-42e2-86ec-a78bf7eeef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, dropout_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(\n",
    "            maximum_position_encoding, d_model)\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.dec_layers = [TransformerBlock(\n",
    "            d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)]\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "        x = self.embedding(x)\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, training=training, mask=look_ahead_mask)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f56cb75c-fdca-4ca5-b13a-2fffa0db71d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "                 input_vocab_size, target_vocab_size, maximum_position_encoding, dropout_rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
    "                               input_vocab_size, maximum_position_encoding, dropout_rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
    "                               target_vocab_size, maximum_position_encoding, dropout_rate)\n",
    "        self.final_layer = Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs, training=False, look_ahead_mask=None, padding_mask=None):\n",
    "        inp, tar = inputs\n",
    "        enc_output = self.encoder(inp, training=training, mask=padding_mask)\n",
    "        dec_output, _ = self.decoder(tar, enc_output, training=training,\n",
    "                                     look_ahead_mask=look_ahead_mask, padding_mask=padding_mask)\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a595bb53-ffac-4243-9c3f-312e1d8506f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.depth = d_model // num_heads\n",
    "        \n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth)\"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, q, k, v, mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        # Linear projections\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        \n",
    "        # Split heads\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)  # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        \n",
    "        # Scale\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "        \n",
    "        # Apply mask (if provided)\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "        \n",
    "        # Softmax\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        output = tf.matmul(attention_weights, v)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        output = tf.transpose(output, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "        concat_attention = tf.reshape(output, (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.dense(concat_attention)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2f2ee68-58df-49bd-b91b-701417f9ac32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 50, 8000)\n"
     ]
    }
   ],
   "source": [
    "# Defining Custom Parameters.\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "dff = 512\n",
    "input_vocab_size = 8500\n",
    "target_vocab_size = 8000\n",
    "maximum_position_encoding = 10000\n",
    "dropout_rate = 0.1\n",
    "\n",
    "transformer = Transformer(\n",
    "    num_layers,\n",
    "    d_model,\n",
    "    num_heads,\n",
    "    dff,\n",
    "    input_vocab_size,\n",
    "    target_vocab_size,\n",
    "    maximum_position_encoding,\n",
    "    dropout_rate\n",
    ")\n",
    "\n",
    "inputs = tf.random.uniform((64, 50), dtype=tf.int64,\n",
    "                           minval=0, maxval=input_vocab_size)\n",
    "targets = tf.random.uniform(\n",
    "    (64, 50), dtype=tf.int64, minval=0, maxval=target_vocab_size)\n",
    "\n",
    "look_ahead_mask = None\n",
    "padding_mask = None\n",
    "\n",
    "output = transformer((inputs, targets), training=True,\n",
    "                     look_ahead_mask=look_ahead_mask, padding_mask=padding_mask)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc83231-437a-4017-a6bd-0a26ef7f3378",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
